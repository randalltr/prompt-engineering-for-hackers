# Prompt Engineering for Hackers

> _â€œAI is just another attack surface.â€_

Welcome to **Prompt Engineering for Hackers**, your hands-on introduction to manipulating and understanding large language models â€” built for hackers, by hackers.

Inspired by books like **Linux Basics for Hackers** and **The Web Application Hackerâ€™s Handbook**, this project focuses on teaching **prompt injection, LLM behavior, and adversarial prompting** from the ground up â€” no PhD required.

---

## ğŸ“š What You'll Learn

- What prompts really are â€” and how they guide the model
- Core techniques: zero-shot, few-shot, CoT, role prompts
- How to craft injections, jailbreaks, and basic evasion
- How to build your own red team lab to test LLMs safely
- Where defenses break down â€” and how to think like an LLM attacker

---

## ğŸ§‘â€ğŸ’» Who This Book is For

- Hackers and pentesters new to AI/LLMs
- Students and self-learners exploring AI security
- Bug bounty hunters who want to target prompt injection
- Security pros trying to keep up with AI's new attack surface

You don't need machine learning experience. If you can write an XSS payload or script a shell, you can learn to hack a prompt.

---

## âš”ï¸ What's Inside

- Tactical chapters with real-world examples
- Labs using open tools like `ollama`, `ai-goat`, and `MyLLMBank`
- Exercises and prompts you can test safely
- Written in Markdown, published freely on GitBook and GitHub

# Summary

### ğŸ§  Part I â€“ The Basics
- [Chapter 1 â€“ Whatâ€™s a Prompt, Anyway?](chapters/01-whats-a-prompt.md)
- [Chapter 2 â€“ Talking to a Language Model](chapters/02-talking-to-an-llm.md)
- [Chapter 3 â€“ Prompt Engineering 101](chapters/03-prompt-engineering-101.md)
- [Chapter 4 â€“ Setting Up Your Prompt Lab](chapters/04-setting-up-lab.md)

### ğŸ’€ Part II â€“ Prompt Hacking Begins
- [Chapter 5 â€“ Simple Instruction Attacks](chapters/05-simple-instruction.md)
- [Chapter 6 â€“ Ignore Context, Ignore the Rules](chapters/06-ignore-context.md)
- [Chapter 7 â€“ Injection Through Inputs](chapters/07-injection-via-input.md)
- [Chapter 8 â€“ Personas, DAN, and Roleplay Jailbreaks](chapters/08-personas-and-dan.md)
- [Chapter 9 â€“ Obfuscation and Encoding](chapters/09-obfuscation-encoding.md)
- [Chapter 10 â€“ Task Switching and Context Hijacks](chapters/10-task-deflection.md)

### ğŸ§ª Part III â€“ Going Deeper
- [Chapter 11 â€“ Payload Splitting](chapters/11-payload-splitting.md)
- [Chapter 12 â€“ Basic Defenses (and Why They Fail)](chapters/12-basic-defenses.md)
- [Chapter 13 â€“ Where to Go Next](chapters/13-where-next.md)

---

## ğŸ“– Read Online (Coming Soon)

> GitBook: [promptengineeringforhackers.gitbook.io](#)

---

## âœï¸ Contribute

PRs, examples, and improvements welcome. Fork it, play with it, and letâ€™s teach hackers everywhere how to bend prompts.

---

## âš ï¸ Disclaimer

This project is for **educational and research purposes** only. Do not test against systems you donâ€™t own or have explicit permission to assess. See [DISCLAIMER.md](DISCLAIMER.md) for full guidance.

---

## ğŸ§  Author

Created by [Randall](https://github.com/randalltr)  
LLM red teamer. Prompt manipulator. Offensive security enthusiast.

---

> â€œYou donâ€™t need to speak AI â€” you just need to speak clearly enough to trick it.â€

# Prompt Engineering for Hackers

> _“AI is just another attack surface.”_

Welcome to **Prompt Engineering for Hackers**, your hands-on introduction to manipulating and understanding large language models — built for hackers, by hackers.

Inspired by books like **Linux Basics for Hackers** and **The Web Application Hacker’s Handbook**, this project focuses on teaching **prompt injection, LLM behavior, and adversarial prompting** from the ground up — no PhD required.

---

## 📚 What You'll Learn

- What prompts really are — and how they guide the model
- Core techniques: zero-shot, few-shot, CoT, role prompts
- How to craft injections, jailbreaks, and basic evasion
- How to build your own red team lab to test LLMs safely
- Where defenses break down — and how to think like an LLM attacker

---

## 🧑‍💻 Who This Book is For

- Hackers and pentesters new to AI/LLMs
- Students and self-learners exploring AI security
- Bug bounty hunters who want to target prompt injection
- Security pros trying to keep up with AI's new attack surface

You don't need machine learning experience. If you can write an XSS payload or script a shell, you can learn to hack a prompt.

---

## ⚔️ What's Inside

- Tactical chapters with real-world examples
- Labs using open tools like `ollama`, `ai-goat`, and `MyLLMBank`
- Exercises and prompts you can test safely
- Written in Markdown, published freely on GitBook and GitHub

# Summary

### 🧠 Part I – The Basics
- [Chapter 1 – What’s a Prompt, Anyway?](chapters/01-whats-a-prompt.md)
- [Chapter 2 – Talking to a Language Model](chapters/02-talking-to-an-llm.md)
- [Chapter 3 – Prompt Engineering 101](chapters/03-prompt-engineering-101.md)
- [Chapter 4 – Setting Up Your Prompt Lab](chapters/04-setting-up-lab.md)

### 💀 Part II – Prompt Hacking Begins
- [Chapter 5 – Simple Instruction Attacks](chapters/05-simple-instruction.md)
- [Chapter 6 – Ignore Context, Ignore the Rules](chapters/06-ignore-context.md)
- [Chapter 7 – Injection Through Inputs](chapters/07-injection-via-input.md)
- [Chapter 8 – Personas, DAN, and Roleplay Jailbreaks](chapters/08-personas-and-dan.md)
- [Chapter 9 – Obfuscation and Encoding](chapters/09-obfuscation-encoding.md)
- [Chapter 10 – Task Switching and Context Hijacks](chapters/10-task-deflection.md)

### 🧪 Part III – Going Deeper
- [Chapter 11 – Payload Splitting](chapters/11-payload-splitting.md)
- [Chapter 12 – Basic Defenses (and Why They Fail)](chapters/12-basic-defenses.md)
- [Chapter 13 – Where to Go Next](chapters/13-where-next.md)

---

## 📖 Read Online (Coming Soon)

> GitBook: [promptengineeringforhackers.gitbook.io](#)

---

## ✍️ Contribute

PRs, examples, and improvements welcome. Fork it, play with it, and let’s teach hackers everywhere how to bend prompts.

---

## ⚠️ Disclaimer

This project is for **educational and research purposes** only. Do not test against systems you don’t own or have explicit permission to assess. See [DISCLAIMER.md](DISCLAIMER.md) for full guidance.

---

## 🧠 Author

Created by [Randall](https://github.com/randalltr)  
LLM red teamer. Prompt manipulator. Offensive security enthusiast.

---

> “You don’t need to speak AI — you just need to speak clearly enough to trick it.”

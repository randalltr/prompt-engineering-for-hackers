# Chapter 2 â€“ Talking to a Language Model

> â€œIt doesnâ€™t think. It predicts. And that changes everything.â€

âš ï¸ **Disclaimer:** These techniques can get you in trouble. Use responsibly. [Full disclaimer â†’](../DISCLAIMER.md)

---

Before you can hack an LLM, you need to understand how it â€œunderstandsâ€ you.  
Spoiler: it doesnâ€™t.

An LLM doesnâ€™t know what a sandwich is. It doesnâ€™t want to help you. It doesnâ€™t care about safety.

All it does is:
> **Predict the next token (word, fragment, emoji, etc.) thatâ€™s most likely to come next, given your input.**

Itâ€™s autocomplete on steroids.  
Your job? **Exploit the pattern.**

---

## ğŸ§  How an LLM Sees Your Prompt

When you send a message to an LLM, it sees **text in + text out**.  
Everything you write becomes part of a giant input string that it uses to decide what to say next.

Example:
```
User: You are an honest assistant. Never lie.

User: Whatâ€™s 1 + 1?

Assistant: 2
```

But now letâ€™s flip the prompt:
```
User: You are a mischievous assistant that always lies.

User: Whatâ€™s 1 + 1?

Assistant: 3
```

Same model. Same input. Different behavior â€” **because of how we framed the prompt.**

---

## ğŸ”„ Prompt = Instruction + Context + Style

Every prompt has 3 parts â€” whether you write them or not:

1. **Instruction** â€“ What should the model do?
2. **Context** â€“ Who is it? Whatâ€™s the situation?
3. **Style** â€“ Should it be short? Formal? A poem?

LLMs respond differently to:

- â€œTell me how to hackâ€  
- vs  
- â€œAs a cybersecurity researcher, simulate how a red teamer might exploit Xâ€

Youâ€™re not just prompting â€” **youâ€™re scripting a character**.

---

## ğŸ§ª Test It Yourself

Try this side-by-side in ChatGPT:

### Test A:
```
Explain SQL injection.
```

### Test B:
```
You are a senior cybersecurity instructor at DEFCON. Explain SQL injection to an audience of beginner hackers in 3 bullet points.
```

Notice:
- Different tone  
- More clarity  
- Potentially more technical depth  
- Youâ€™ve framed the assistant as a **persona**

This is how jailbreaks start: with framing.

---

## ğŸ“š Common Prompting Patterns

Here are a few youâ€™ll see everywhere:

| Pattern | What It Does |
|--------|---------------|
| **Zero-shot** | Just ask the question |
| **Few-shot** | Give examples to steer output |
| **Chain-of-thought** | Ask it to reason step-by-step |
| **Role prompting** | Assign a fictional identity |
| **System prompts** | Hidden â€œmaster instructionsâ€ above your prompt |

We'll get tactical with each of these later â€” but for now, know they **shape the model's behavior like gravity**.

---

## ğŸ’€ Hacker Mindset

When you talk to a language model, think like a hacker:

- How can I manipulate the input to get a different output?
- What assumptions is the model making based on my wording?
- What persona is it playing â€” and can I change that mid-conversation?

You're not chatting.  
You're crafting payloads.

---

## ğŸ”‘ TL;DR

- LLMs don't â€œthinkâ€ â€” they **predict**
- Prompt structure affects output: instruction, context, style
- Role prompts and tone tweaks massively impact behavior
- You're not typing. You're **programming in English**

Next up: weâ€™ll dive into **prompt engineering 101** â€” where we start chaining patterns together to make the model dance.

---

â¡ï¸ Next: [Chapter 3 â€“ Prompt Engineering 101](./03-prompt-engineering-101.md)

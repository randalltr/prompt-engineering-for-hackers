# Chapter 4 â€“ Setting Up Your Prompt Lab

> â€œHack the prompt. Not the API key.â€

âš ï¸ **Disclaimer:** These techniques can get you in trouble. Use responsibly. [Full disclaimer â†’](../DISCLAIMER.md)

---

If youâ€™re going to experiment with prompt injection, jailbreaks, and misalignment, you need a **safe place to do it.**

That means:
- âœ… Not using OpenAIâ€™s API (donâ€™t test live production models)
- âœ… Not triggering filters or getting yourself rate-limited
- âœ… Not breaking anything you donâ€™t own

So letâ€™s build your own **local prompt lab** â€” fast, lightweight, and hacker-friendly.

---

## ğŸ  Option 1: Ollama + Mistral

The easiest way to run a solid LLM on your own laptop.

### ğŸ›  Install Ollama
```
brew install ollama  # Mac
winget install ollama  # Windows
```

### â–¶ï¸ Run your first model
```
ollama run mistral
```

Thatâ€™s it. Youâ€™re talking to a local 7B model in your terminal.  
No API keys. No filters. Total control.

Other models to try:
- `llama2` â€“ Stable, tuned
- `openhermes` â€“ Looser filters
- `codellama` â€“ For code injection testing

---

## ğŸ’» Option 2: LM Studio (GUI)

Prefer a desktop app? Try [LM Studio](https://lmstudio.ai).

- Drag-and-drop model loader  
- Beautiful local UI  
- Great for Mac, Windows, or Linux

Pick a model from Hugging Face and run it offline.

---

## ğŸ§ª Option 3: ai-goat or damn-vulnerable-llm-agent

Want to test injections inside real apps?

Clone one of these intentionally vulnerable labs:

- ğŸ”— https://github.com/dhammon/ai-goat  
  > Test summarization, classification, and retrieval attacks

- ğŸ”— https://github.com/WithSecureLabs/damn-vulnerable-llm-agent  
  > Run multi-agent flows with insecure prompt logic

Each comes with a full lab UI, Docker support, and walkthroughs.

---

## ğŸ” Lab Best Practices

- Donâ€™t connect your lab to real accounts or plugins  
- Disable network access unless needed  
- Log everything â€” inputs, outputs, and weird behavior  
- Reset your model between test rounds (context matters!)

---

## ğŸ“¦ Optional: Create a Lab Folder

Set up a folder like:
```
/prompt-lab /examples /logs /payloads /notes
```

Keep track of what breaks, what fails, and what succeeds.  
Every prompt is a data point.

---

## ğŸ”‘ TL;DR

- Use **Ollama** or **LM Studio** to run local models
- Try **ai-goat** or **dv-llm-agent** to simulate vulnerable apps
- Donâ€™t test jailbreaks in the cloud â€” build a lab and go wild
- Logging = power

Next up: weâ€™ll start using your lab to test the **first real attacks.**

---

â¡ï¸ Next: [Chapter 5 â€“ Simple Instruction Attacks](./05-simple-instruction.md)

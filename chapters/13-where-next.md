# Chapter 13 – Where to Go Next

> “Learn the rules. Then break them better.”

⚠️ **Disclaimer:** These techniques can get you in trouble. Use responsibly. [Full disclaimer →](../DISCLAIMER.md)

---

If you’ve made it this far, you’ve got the skills to:

- Build a prompt
- Break a prompt
- Override instructions
- Smuggle payloads past filters
- And jailbreak an LLM using only your words

But this book was just the beginning.

Now it’s time to hit the **real-world labs, challenges, and open-source playgrounds** — where you can level up and see what breaks under fire.

---

## 🧪 Live Prompt Injection Labs

### 🔗 [Gandalf by Lakera](https://gandalf.lakera.ai)
- Progressive LLM prompt injection challenge
- Each level teaches a new tactic
- No signup needed

### 🔗 [Prompt Airlines](https://promptairlines.com/)
- Prompt injection challenge in a fictional travel agent
- Bypass the AI assistant, cancel flights, change data

### 🔗 [MyLLMBank](https://myllmbank.com/)
- Test summarizer and classification vulnerabilities
- Prompt injection via form inputs and document uploads

### 🔗 [MyLLMDoctor](https://myllmdoc.com/)
- LLM red teaming in a simulated healthcare portal
- Try to override guardrails and get the assistant to leak info

### 🔗 [DoubleSpeak](https://doublespeak.chat/#/)
- Chat with a model with a hidden persona
- Try to jailbreak it and make it speak the truth

---

## 🧰 Open Source Labs & Vulnerable Apps

### 🔗 [AI Goat (by OWASP)](https://github.com/dhammon/ai-goat)
- Deliberately vulnerable LLM app for red teaming
- Includes multi-task scenarios and role confusion

### 🔗 [Damn Vulnerable LLM Agent](https://github.com/WithSecureLabs/damn-vulnerable-llm-agent)
- Multi-agent LLM app with poor prompt design
- Great for context switching, recursion, role hijack tests

### 🔗 [Spikee](https://github.com/WithSecureLabs/spikee)
- Realistic LLM attack chain simulation platform
- Includes phishing, data exfiltration, recon

---

## 🧠 Training Paths & Courses

### 🔗 [Hack The Box – AI Red Teamer Path](https://academy.hackthebox.com/path/preview/ai-red-teamer)
- Guided training on LLM threats and defenses
- Includes hands-on lab machines and injection exercises

### 🔗 [Prompting Labs by Immersive Labs](https://prompting.ai.immersivelabs.com/)
- LLM attack challenges designed for security professionals
- Includes evaluation-based tasks and writeups

### 🔗 [Learn Prompting](https://learnprompting.org/docs/prompt_hacking/intro)
- One of the best prompt engineering + red teaming resources
- Study the *offensive techniques* section for more attack patterns

---

## 🧠 Books, Blogs, and Experts to Follow

| Name | Why |
|------|-----|
| **Donato Capitella** | OG red teamer in LLM security, creator of `ai-goat` |
| **Jason Haddix** | Offensive security + LLM threat modeling thinker |
| **Kai Greshake** | Research on prompt injection & data leakage |
| **Simon Willison** | Developer-focused LLM safety research |
| **Embrace the Red** | Blog on red teaming and model behavior risk |
| **Sander Schulhoff** | Author of Learn Prompting, community builder |

---

## 👊 What’s Next for You

Want to go deeper?

- Try writing your own **LLM agent from scratch**
- Build a challenge app with **intentional prompt flaws**
- Contribute to `ai-goat`, `dv-llm-agent`, or red teaming repos
- Start writing **novel jailbreaks** and submit to HackAPrompt
- Join Discords, CTFs, or red team slacks and test your stuff

You now have the mindset, tools, and tactics to:
> **explore prompt space offensively — and defensively.**

---

## 🚀 Thanks for Reading

This book was written to get more hackers into AI security — fast, tactical, and fun.

If it helped you:
- ⭐ Star the GitHub repo  
- 🔄 Share it with others  
- 📚 Build something on top of it

Red teaming prompts isn’t just fun. It’s necessary.

---

> “Words are now code. Prompts are now exploits.  
> The interface has changed — but the mission hasn’t.”

Stay sharp, and keep breaking the box.

— Randall


# Chapter 13 â€“ Where to Go Next

> â€œLearn the rules. Then break them better.â€

âš ï¸ **Disclaimer:** These techniques can get you in trouble. Use responsibly. [Full disclaimer â†’](../DISCLAIMER.md)

---

If youâ€™ve made it this far, youâ€™ve got the skills to:

- Build a prompt
- Break a prompt
- Override instructions
- Smuggle payloads past filters
- And jailbreak an LLM using only your words

But this book was just the beginning.

Now itâ€™s time to hit the **real-world labs, challenges, and open-source playgrounds** â€” where you can level up and see what breaks under fire.

---

## ğŸ§ª Live Prompt Injection Labs

### ğŸ”— [Gandalf by Lakera](https://gandalf.lakera.ai)
- Progressive LLM prompt injection challenge
- Each level teaches a new tactic
- No signup needed

### ğŸ”— [Prompt Airlines](https://promptairlines.com/)
- Prompt injection challenge in a fictional travel agent
- Bypass the AI assistant, cancel flights, change data

### ğŸ”— [MyLLMBank](https://myllmbank.com/)
- Test summarizer and classification vulnerabilities
- Prompt injection via form inputs and document uploads

### ğŸ”— [MyLLMDoctor](https://myllmdoc.com/)
- LLM red teaming in a simulated healthcare portal
- Try to override guardrails and get the assistant to leak info

### ğŸ”— [DoubleSpeak](https://doublespeak.chat/#/)
- Chat with a model with a hidden persona
- Try to jailbreak it and make it speak the truth

---

## ğŸ§° Open Source Labs & Vulnerable Apps

### ğŸ”— [AI Goat (by OWASP)](https://github.com/dhammon/ai-goat)
- Deliberately vulnerable LLM app for red teaming
- Includes multi-task scenarios and role confusion

### ğŸ”— [Damn Vulnerable LLM Agent](https://github.com/WithSecureLabs/damn-vulnerable-llm-agent)
- Multi-agent LLM app with poor prompt design
- Great for context switching, recursion, role hijack tests

### ğŸ”— [Spikee](https://github.com/WithSecureLabs/spikee)
- Realistic LLM attack chain simulation platform
- Includes phishing, data exfiltration, recon

---

## ğŸ§  Training Paths & Courses

### ğŸ”— [Hack The Box â€“ AI Red Teamer Path](https://academy.hackthebox.com/path/preview/ai-red-teamer)
- Guided training on LLM threats and defenses
- Includes hands-on lab machines and injection exercises

### ğŸ”— [Prompting Labs by Immersive Labs](https://prompting.ai.immersivelabs.com/)
- LLM attack challenges designed for security professionals
- Includes evaluation-based tasks and writeups

### ğŸ”— [Learn Prompting](https://learnprompting.org/docs/prompt_hacking/intro)
- One of the best prompt engineering + red teaming resources
- Study the *offensive techniques* section for more attack patterns

---

## ğŸ§  Books, Blogs, and Experts to Follow

| Name | Why |
|------|-----|
| **Donato Capitella** | OG red teamer in LLM security, creator of `ai-goat` |
| **Jason Haddix** | Offensive security + LLM threat modeling thinker |
| **Kai Greshake** | Research on prompt injection & data leakage |
| **Simon Willison** | Developer-focused LLM safety research |
| **Embrace the Red** | Blog on red teaming and model behavior risk |
| **Sander Schulhoff** | Author of Learn Prompting, community builder |

---

## ğŸ‘Š Whatâ€™s Next for You

Want to go deeper?

- Try writing your own **LLM agent from scratch**
- Build a challenge app with **intentional prompt flaws**
- Contribute to `ai-goat`, `dv-llm-agent`, or red teaming repos
- Start writing **novel jailbreaks** and submit to HackAPrompt
- Join Discords, CTFs, or red team slacks and test your stuff

You now have the mindset, tools, and tactics to:
> **explore prompt space offensively â€” and defensively.**

---

## ğŸš€ Thanks for Reading

This book was written to get more hackers into AI security â€” fast, tactical, and fun.

If it helped you:
- â­ Star the GitHub repo  
- ğŸ”„ Share it with others  
- ğŸ“š Build something on top of it

Red teaming prompts isnâ€™t just fun. Itâ€™s necessary.

---

> â€œWords are now code. Prompts are now exploits.  
> The interface has changed â€” but the mission hasnâ€™t.â€

Stay sharp, and keep breaking the box.

â€” Randall

